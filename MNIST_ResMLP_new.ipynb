{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST by ResMLP\n",
    "##### written by LarryHYQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import some necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from einops.layers.torch import Rearrange\n",
    "from tqdm.notebook import tqdm\n",
    "import IPython.display as display\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the ResMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Aff(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.alpha = nn.Parameter(torch.ones([1, 1, dim]))\n",
    "        self.beta = nn.Parameter(torch.zeros([1, 1, dim]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x * self.alpha + self.beta\n",
    "        return x\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class MLPblock(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, num_patch, mlp_dim, dropout = 0., init_values=1e-4):\n",
    "        super().__init__()\n",
    "        self.pre_affine = Aff(dim)\n",
    "        self.token_mix = nn.Sequential(\n",
    "            Rearrange('b n d -> b d n'),\n",
    "            nn.Linear(num_patch, num_patch),\n",
    "            Rearrange('b d n -> b n d'),\n",
    "        )\n",
    "        self.ff = nn.Sequential(\n",
    "            FeedForward(dim, mlp_dim, dropout),\n",
    "        )\n",
    "        self.post_affine = Aff(dim)\n",
    "        self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n",
    "        self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pre_affine(x)\n",
    "        x = x + self.gamma_1 * self.token_mix(x)\n",
    "        x = self.post_affine(x)\n",
    "        x = x + self.gamma_2 * self.ff(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResMLP(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, dim, num_classes, patch_size, image_size, depth, mlp_dim):\n",
    "        super().__init__()\n",
    "        assert image_size % patch_size == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "        self.num_patch =  (image_size// patch_size) ** 2\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, dim, patch_size, patch_size),\n",
    "            Rearrange('b c h w -> b (h w) c'),\n",
    "        )\n",
    "        self.mlp_blocks = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.mlp_blocks.append(MLPblock(dim, self.num_patch, mlp_dim))\n",
    "        self.affine = Aff(dim)\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.to_patch_embedding(x)\n",
    "        for mlp_block in self.mlp_blocks:\n",
    "            x = mlp_block(x)\n",
    "        x = self.affine(x)\n",
    "        x = x.mean(dim=1)\n",
    "        return self.mlp_head(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting some Paeameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr = 0.001  # learning rate\n",
    "batch_size_train = 600  # batch_size for train\n",
    "batch_size_test = 1000  # batch_size for test\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "all_epochs = 200\n",
    "writer = SummaryWriter(\"runs/MNIST_ResMLP_experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting GPU or CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    global trainset, trainloader, testset, testloader\n",
    "    print('==> Preparing data..')\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    ])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    ])\n",
    "\n",
    "    trainset = torchvision.datasets.MNIST(\n",
    "        root='./data', train=True, download=True, transform=transform_train)\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        trainset, batch_size=batch_size_train, shuffle=True, num_workers=2)\n",
    "\n",
    "    testset = torchvision.datasets.MNIST(\n",
    "        root='./data', train=False, download=True, transform=transform_test)\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=batch_size_test, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_model():\n",
    "    global net, optimizer, criterion, start_epoch, best_acc\n",
    "    print('==> Building model..')\n",
    "    net = ResMLP(in_channels=1, image_size=28, patch_size=7, num_classes=10, dim=384, depth=2, mlp_dim=384*4)\n",
    "    net = net.to(device)\n",
    "    if device == 'cuda':\n",
    "        net = torch.nn.DataParallel(net)\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "    resume_flag = input(\"Whether to continue training?(Y/N)\\n\")\n",
    "\n",
    "    if (resume_flag == \"Y\") or (resume_flag == \"y\"):\n",
    "        # Load checkpoint.\n",
    "        print('==> Resuming from checkpoint..')\n",
    "        assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
    "        last_checkpoint = torch.load('./checkpoint/mnist_resmlp_last.pth')\n",
    "        best_checkpoint = torch.load('./checkpoint/mnist_resmlp_best.pth')\n",
    "        net.load_state_dict(last_checkpoint['net'])\n",
    "        best_acc = best_checkpoint['acc']\n",
    "        start_epoch = last_checkpoint['epoch'] + 1\n",
    "\n",
    "    criterion = F.cross_entropy\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot on TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tb():\n",
    "    images, labels = next(iter(trainloader))\n",
    "    grid = torchvision.utils.make_grid(images)\n",
    "    writer.add_image(\"image\", grid)\n",
    "    writer.add_graph(net, images)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    pbar = tqdm(enumerate(trainloader), desc=f'TRAIN Epoch {epoch}',total=len(trainloader))\n",
    "    for batch_idx, (inputs, targets) in pbar:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        pbar.set_postfix({\"Loss\":f\"{(train_loss/(batch_idx+1)):.3f}\",\"Acc\":f\"{100*correct/total:.3f} ({correct}/{total})\"})\n",
    "    writer.add_scalar(\"TRAIN/Loss\", (train_loss/(batch_idx+1)), epoch)\n",
    "    writer.add_scalar(\"TRAIN/Acuracy\", (correct/total), epoch)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(enumerate(testloader), desc=f'TEST Epoch {epoch}',total=len(testloader))\n",
    "        for batch_idx, (inputs, targets) in pbar:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            pbar.set_postfix({\"Loss\":f\"{(test_loss/(batch_idx+1)):.3f}\",\"Acc\":f\"{100*correct/total:.3f} ({correct}/{total})\"})\n",
    "        writer.add_scalar(\"TEST/Loss\", (test_loss/(batch_idx+1)), epoch)\n",
    "        writer.add_scalar(\"TEST/Accuracy\", (correct/total), epoch)   \n",
    "        writer.close()\n",
    "        \n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    print('Saving the last..')\n",
    "    state = {\n",
    "        'net': net.state_dict(),\n",
    "        'acc': acc,\n",
    "        'epoch': epoch,\n",
    "    }\n",
    "\n",
    "    if not os.path.isdir('checkpoint'):\n",
    "        os.mkdir('checkpoint')\n",
    "    torch.save(state, './checkpoint/mnist_resmlp_last.pth')\n",
    "    print(\"The last Acc:\" + str(acc))\n",
    "    if acc > best_acc:\n",
    "        print('Saving the best..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    " \n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        torch.save(state, './checkpoint/mnist_resmlp_best.pth')\n",
    "        best_acc = acc\n",
    "    print(\"The best Acc:\" + str(best_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "==> Building model..\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Whether to continue training?(Y/N)\n",
      " Y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Resuming from checkpoint..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\miniconda3\\lib\\site-packages\\einops\\einops.py:179: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  axes_lengths[unknown_axis] = length // known_product\n"
     ]
    }
   ],
   "source": [
    "prepare_data()\n",
    "set_model()\n",
    "plot_tb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08f31f983f894474b204fe437ebf48c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TRAIN Epoch 96:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in range(start_epoch, start_epoch + all_epochs):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "    display.clear_output(wait=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
